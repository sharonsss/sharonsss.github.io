<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Elasticdump 踩坑记录</title>
    <link href="/2020/04/02/elasticdump%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/04/02/elasticdump%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<blockquote><p>感觉这几天是自己的智商低谷。😭</p></blockquote><h1 id="Elasticdump-安装"><a href="#Elasticdump-安装" class="headerlink" title="Elasticdump 安装"></a>Elasticdump 安装</h1><p>为了解决远程ES数据库导入到本地ES的问题，今天在网上查了一天资料。了解到ELK中的Logstash可以实现这个需求，同时Logstash似乎还隐藏着更多其他炫酷功能，包括我后面可能要用到的web接口。<a id="more"></a> 感觉到这是一个稍微大点的功能，最终决定后面专门腾出时间来研究。</p><p>于是就找到了另外一个非常轻量级的ElasticSearch插件————Elasticdump，专门解决ES数据导入导出的问题。“dump”也是个非常形象的词了，有些简单粗暴，就跟它的实现一样。</p><p>安装Elasticdump很简单，mac上直接 <code>npm install elasticdump</code> 就好了。也可以全剧安装 <code>npm install elasticdump -g</code> 。</p><h1 id="将远程-ES-数据导出到本地-JSON-文件"><a href="#将远程-ES-数据导出到本地-JSON-文件" class="headerlink" title="将远程 ES 数据导出到本地 JSON 文件"></a>将远程 ES 数据导出到本地 JSON 文件</h1><p>这里演示的是我把远程ES上的数据导出到本地的JSON文件。最初的想法是，把远程ES数据导到本地文件，再把文件导入本地的ES。此时，我并没有想到远程ES和本地ES可以直接导入导出，脑子卡在了本地文件里。😓</p><p>实际output部分为本地文件路径，具体如下：</p><pre><code># 格式：$ elasticdump --input {protocol}://{host}:{port}/{index} --output ./test_index.json# 例子：将ES中的test_index 中的索引导出# 导出当前索引的mapping结构$ elasticdump --input http://192.168.56.104:9200/test_index \--output ./test_index_mapping.json --type=mapping# 导出当前索引下的所有真实数据$ elasticdump --input http://192.168.56.104:9200/test_index \--output ./test_index.json --type=data</code></pre><h1 id="将远程的-ES-数据导出到本地-ES"><a href="#将远程的-ES-数据导出到本地-ES" class="headerlink" title="将远程的 ES 数据导出到本地 ES"></a>将远程的 ES 数据导出到本地 ES</h1><p>本地JSON导出之后，终于想到elasticdump明明可以连接远程和本地ES。。。</p><h2 id="无账号密码的情况下导出"><a href="#无账号密码的情况下导出" class="headerlink" title="无账号密码的情况下导出"></a>无账号密码的情况下导出</h2><p>如果远程和本地的ES都不需要账号密码访问权限，相对来说就比较容易，直接遵循下面的格式就可以。</p><p>官方提供的数据迁移示例：</p><pre><code># 拷贝analyzer分词elasticdump \  --input=http://production.es.com:9200/my_index \  --output=http://staging.es.com:9200/my_index \  --type=analyzer</code></pre><pre><code># 拷贝映射elasticdump \  --input=http://production.es.com:9200/my_index \  --output=http://staging.es.com:9200/my_index \  --type=mapping</code></pre><pre><code># 拷贝数据elasticdump \  --input=http://production.es.com:9200/my_index \  --output=http://staging.es.com:9200/my_index \  --type=data</code></pre><h2 id="设置账号密码登录导出"><a href="#设置账号密码登录导出" class="headerlink" title="设置账号密码登录导出"></a>设置账号密码登录导出</h2><p><strong>据说 elasticdump 提供给了–httpAuthFile 参数来做认证</strong></p><pre><code>--httpAuthFile      When using http auth provide credentials in ini file in form                    `user=&lt;username&gt;                    password=&lt;password&gt;`</code></pre><p>只需要写一个ini文件 ，文件中写入用户名和密码就可以了，不过这个方法我还没有试。</p><p>另外一个好的方法是，<strong>在–input参数和–output参数的的url中添加账号密码</strong>。</p><p>例如：</p><pre><code>$ elasticdump --input http://username:passowrd@production.es.com:9200/my_index \--output http://username:password@staging.es.com:9200/my_index \--type=data</code></pre><p>导出mapping</p><pre><code>$ elasticdump --input http://username:password@host:9200/data_index \--output ./test_index.json --type=mapping</code></pre><p>导出所有data</p><pre><code>$ elasticdump --input http://username:password@host:9200/data_index \--output ./test_index_data.json --type=data</code></pre><h2 id="远程-ES-需要账号密码，本地不需要"><a href="#远程-ES-需要账号密码，本地不需要" class="headerlink" title="远程 ES 需要账号密码，本地不需要"></a>远程 ES 需要账号密码，本地不需要</h2><p><strong>上面看起来行云流水一般就能搞定，事实上，对小白来说，好大的坑啊。。。</strong></p><p>折腾完上面的远程ES导出到本地JSON之外，突然发现可以直接从远程ES导入本地ES，为什么我还要拐个弯导出个本地文件，服了我自己的智商。。。</p><p><img src="/img/zhishang1.jpg" srcset="/img/loading.gif" alt="智商"></p><p>由于本地ES没有任何配置，于是我按照示例，最初想到了这样：</p><pre><code>$ elasticdump --input http://username:password@0.0.0.0:9200/data_index \--output http://localhost:9200/data_index --type=mapping</code></pre><p>报错：</p><pre><code>Error Emitted =&gt; {&quot;root_cause&quot;:[{&quot;type&quot;:&quot;mapper_parsing_exception&quot;,&quot;reason&quot;:&quot;Root mapping definition has unsupported parameters:  [kis_data_index_type : {properties={eng_exam_res={type=keyword}, item_state={type=keyword}, first_level={type=text}, chin_exam={analyzer=ik_max_word, type=text}, chinese_item={analyzer=ik_max_word, type=text}, xremark={type=keyword}, eng_synonym={type=keyword}, eng_exam={type=text}, contributor={type=text}, third_level={type=text}, chin_exam_res={type=keyword}, id={type=long}, remark_str={type=keyword}, chin_define_res={type=keyword}, chin_synonym={type=text}, eng_abbr={type=text}, contributor ={type=text, fields={keyword={ignore_above=256, type=keyword}}}, query={properties={match_all={type=object}}}, source_type={type=keyword}, english_item={analyzer=english, type=text}, second_level={type=text}, eng_define_res={type=keyword}, eng_define={type=text}, picture_res={type=keyword}, chin_abbr={type=text}, chin_define={type=text}}}]&quot;}],&quot;type&quot;:&quot;mapper_parsing_exception&quot;,&quot;reason&quot;:&quot;Root mapping definition has unsupported parameters:  [kis_data_index_type : {properties={eng_exam_res={type=keyword}, item_state={type=keyword}, first_level={type=text}, chin_exam={analyzer=ik_max_word, type=text}, chinese_item={analyzer=ik_max_word, type=text}, xremark={type=keyword}, eng_synonym={type=keyword}, eng_exam={type=text}, contributor={type=text}, third_level={type=text}, chin_exam_res={type=keyword}, id={type=long}, remark_str={type=keyword}, chin_define_res={type=keyword}, chin_synonym={type=text}, eng_abbr={type=text}, contributor ={type=text, fields={keyword={ignore_above=256, type=keyword}}}, query={properties={match_all={type=object}}}, source_type={type=keyword}, english_item={analyzer=english, type=text}, second_level={type=text}, eng_define_res={type=keyword}, eng_define={type=text}, picture_res={type=keyword}, chin_abbr={type=text}, chin_define={type=text}}}]&quot;}Error Emitted =&gt; {&quot;root_cause&quot;:[{&quot;type&quot;:&quot;mapper_parsing_exception&quot;,&quot;reason&quot;:&quot;Root mapping definition has unsupported parameters:  [kis_data_index_type : {properties={eng_exam_res={type=keyword}, item_state={type=keyword}, first_level={type=text}, chin_exam={analyzer=ik_max_word, type=text}, chinese_item={analyzer=ik_max_word, type=text}, xremark={type=keyword}, eng_synonym={type=keyword}, eng_exam={type=text}, contributor={type=text}, third_level={type=text}, chin_exam_res={type=keyword}, id={type=long}, remark_str={type=keyword}, chin_define_res={type=keyword}, chin_synonym={type=text}, eng_abbr={type=text}, contributor ={type=text, fields={keyword={ignore_above=256, type=keyword}}}, query={properties={match_all={type=object}}}, source_type={type=keyword}, english_item={analyzer=english, type=text}, second_level={type=text}, eng_define_res={type=keyword}, eng_define={type=text}, picture_res={type=keyword}, chin_abbr={type=text}, chin_define={type=text}}}]&quot;}],&quot;type&quot;:&quot;mapper_parsing_exception&quot;,&quot;reason&quot;:&quot;Root mapping definition has unsupported parameters:  [kis_data_index_type : {properties={eng_exam_res={type=keyword}, item_state={type=keyword}, first_level={type=text}, chin_exam={analyzer=ik_max_word, type=text}, chinese_item={analyzer=ik_max_word, type=text}, xremark={type=keyword}, eng_synonym={type=keyword}, eng_exam={type=text}, contributor={type=text}, third_level={type=text}, chin_exam_res={type=keyword}, id={type=long}, remark_str={type=keyword}, chin_define_res={type=keyword}, chin_synonym={type=text}, eng_abbr={type=text}, contributor ={type=text, fields={keyword={ignore_above=256, type=keyword}}}, query={properties={match_all={type=object}}}, source_type={type=keyword}, english_item={analyzer=english, type=text}, second_level={type=text}, eng_define_res={type=keyword}, eng_define={type=text}, picture_res={type=keyword}, chin_abbr={type=text}, chin_define={type=text}}}]&quot;}</code></pre><p>应该是不能直接用localhost。</p><p>于是我又试了这样：</p><pre><code>$ elasticdump --input http://username:password@0.0.0.0:9200/kis_data_index \--output http://127.0.0.1:9200/data_index --type=mapping</code></pre><p>本机地址，没错吧？</p><p>依然报同样的错误。</p><p>于是，我寻思着是不是应该用电脑无线的IP？(我还加了公司无线网的账号密码。。)</p><pre><code>$ elasticdump --input http://username:password@0.0.0.0:9200/data_index \--output http://OA:password@10.9.1.000:9200/data_index --type=mapping</code></pre><p>这次报错是连不上：</p><pre><code>Thu, 02 Apr 2020 08:01:54 GMT | starting dumpThu, 02 Apr 2020 08:01:54 GMT | got 1 objects from source elasticsearch (offset: 0)Thu, 02 Apr 2020 08:02:14 GMT | Error Emitted =&gt; connect ECONNREFUSED 0.0.0.0:9200Thu, 02 Apr 2020 08:02:14 GMT | Error Emitted =&gt; connect ECONNREFUSED 0.0.0.0:9200Thu, 02 Apr 2020 08:02:14 GMT | Total Writes: 0Thu, 02 Apr 2020 08:02:14 GMT | dump ended with error (get phase) =&gt; Error: connect ECONNREFUSED 0.0.0.0:9200:9200</code></pre><p>把账号密码去了，还是提示连不上。</p><p>这时候我才看到指令最后的 –type=mapping。</p><p>不会是这个指令多余了吧？？</p><p>删掉之后，果然通了。。</p><pre><code>$ elasticdump --input http://username:password@0.0.0.0:9200/data_index \--output http://127.0.0.1:9200/data_index</code></pre><p><strong>妈蛋!</strong><br><strong>被自己气吐血！！</strong></p><pre><code>Thu, 02 Apr 2020 08:24:37 GMT | starting dumpThu, 02 Apr 2020 08:24:37 GMT | got 100 objects from source elasticsearch (offset: 0)Thu, 02 Apr 2020 08:24:38 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:38 GMT | got 100 objects from source elasticsearch (offset: 100)Thu, 02 Apr 2020 08:24:38 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:38 GMT | got 100 objects from source elasticsearch (offset: 200)Thu, 02 Apr 2020 08:24:38 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:38 GMT | got 100 objects from source elasticsearch (offset: 300)Thu, 02 Apr 2020 08:24:38 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:38 GMT | got 100 objects from source elasticsearch (offset: 400)Thu, 02 Apr 2020 08:24:38 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:38 GMT | got 100 objects from source elasticsearch (offset: 500)Thu, 02 Apr 2020 08:24:38 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:42 GMT | got 100 objects from source elasticsearch (offset: 600)Thu, 02 Apr 2020 08:24:42 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:42 GMT | got 100 objects from source elasticsearch (offset: 700)Thu, 02 Apr 2020 08:24:43 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:43 GMT | got 100 objects from source elasticsearch (offset: 800)Thu, 02 Apr 2020 08:24:43 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:43 GMT | got 100 objects from source elasticsearch (offset: 900)Thu, 02 Apr 2020 08:24:43 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:43 GMT | got 100 objects from source elasticsearch (offset: 1000)Thu, 02 Apr 2020 08:24:43 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:48 GMT | got 100 objects from source elasticsearch (offset: 1100)Thu, 02 Apr 2020 08:24:48 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:48 GMT | got 100 objects from source elasticsearch (offset: 1200)Thu, 02 Apr 2020 08:24:48 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:48 GMT | got 100 objects from source elasticsearch (offset: 1300)Thu, 02 Apr 2020 08:24:48 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:48 GMT | got 100 objects from source elasticsearch (offset: 1400)Thu, 02 Apr 2020 08:24:48 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:48 GMT | got 100 objects from source elasticsearch (offset: 1500)Thu, 02 Apr 2020 08:24:48 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:52 GMT | got 100 objects from source elasticsearch (offset: 1600)Thu, 02 Apr 2020 08:24:52 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:53 GMT | got 100 objects from source elasticsearch (offset: 1700)Thu, 02 Apr 2020 08:24:53 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:53 GMT | got 100 objects from source elasticsearch (offset: 1800)Thu, 02 Apr 2020 08:24:53 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:53 GMT | got 100 objects from source elasticsearch (offset: 1900)Thu, 02 Apr 2020 08:24:53 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:24:53 GMT | got 100 objects from source elasticsearch (offset: 2000)Thu, 02 Apr 2020 08:24:53 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:08 GMT | got 100 objects from source elasticsearch (offset: 2100)Thu, 02 Apr 2020 08:25:08 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:08 GMT | got 100 objects from source elasticsearch (offset: 2200)Thu, 02 Apr 2020 08:25:08 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:08 GMT | got 100 objects from source elasticsearch (offset: 2300)Thu, 02 Apr 2020 08:25:08 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:08 GMT | got 100 objects from source elasticsearch (offset: 2400)Thu, 02 Apr 2020 08:25:08 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:08 GMT | got 100 objects from source elasticsearch (offset: 2500)Thu, 02 Apr 2020 08:25:08 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:08 GMT | got 100 objects from source elasticsearch (offset: 2600)Thu, 02 Apr 2020 08:25:08 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:13 GMT | got 100 objects from source elasticsearch (offset: 2700)Thu, 02 Apr 2020 08:25:13 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:13 GMT | got 100 objects from source elasticsearch (offset: 2800)Thu, 02 Apr 2020 08:25:13 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:13 GMT | got 100 objects from source elasticsearch (offset: 2900)Thu, 02 Apr 2020 08:25:13 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:13 GMT | got 100 objects from source elasticsearch (offset: 3000)Thu, 02 Apr 2020 08:25:13 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:13 GMT | got 100 objects from source elasticsearch (offset: 3100)Thu, 02 Apr 2020 08:25:13 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:18 GMT | got 100 objects from source elasticsearch (offset: 3200)Thu, 02 Apr 2020 08:25:18 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:18 GMT | got 100 objects from source elasticsearch (offset: 3300)Thu, 02 Apr 2020 08:25:18 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:18 GMT | got 100 objects from source elasticsearch (offset: 3400)Thu, 02 Apr 2020 08:25:18 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:18 GMT | got 100 objects from source elasticsearch (offset: 3500)Thu, 02 Apr 2020 08:25:18 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:18 GMT | got 100 objects from source elasticsearch (offset: 3600)Thu, 02 Apr 2020 08:25:18 GMT | sent 100 objects to destination elasticsearch, wrote 100Thu, 02 Apr 2020 08:25:23 GMT | got 35 objects from source elasticsearch (offset: 3700)Thu, 02 Apr 2020 08:25:23 GMT | sent 35 objects to destination elasticsearch, wrote 35Thu, 02 Apr 2020 08:25:23 GMT | got 0 objects from source elasticsearch (offset: 3735)Thu, 02 Apr 2020 08:25:23 GMT | Total Writes: 3735Thu, 02 Apr 2020 08:25:23 GMT | dump complete</code></pre><p>不过，我在网上研究的时候，看到示例中都有指定–type=XXX，mapping和data要分别指定。后边需要验证一下不指定–type的情况下，导入到本地ES的文件是不是同时包含了mapping和所有data数据（从导入的记录上看，确实完整导入了3735条数据）。</p>]]></content>
    
    
    <categories>
      
      <category>ElasticSearch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>插件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>知识点（一）：MVVM模型</title>
    <link href="/2020/03/23/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%80/"/>
    <url>/2020/03/23/%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%80/</url>
    
    <content type="html"><![CDATA[<p>在学习Vue的过程中，最先遇到的就是MVVM、耦合、解耦的概念，在这里用“知识点”的分类总结一下。</p><p>知识点的帖子应该会不断增加，积少成多做成一个系列，慢慢充实起来。<a id="more"></a></p><hr><h2 id="MVVM模型"><a href="#MVVM模型" class="headerlink" title="MVVM模型"></a>MVVM模型</h2><p>MVVM是Model-View-ViewModel的缩写，一种前端开发的模型。主要思想是，在前端页面中，把Model用纯JavaScript对象表示，View负责显示，两者做到了最大程度的分离。</p><p>在MVVM架构下，View和Model之间没有直接的联系，而是通过ViewModel来交互，View数据的变化会同步到Model中，Model数据的变化也会立即反应到View上。</p><p><img src="/img/mvvm2.png" srcset="/img/loading.gif" alt="mvvm"></p><br><p>一个MVVM框架和传统JS、jQuery操作DOM的区别是什么？</p><h3 id="传统-JS-操作DOM"><a href="#传统-JS-操作DOM" class="headerlink" title="传统 JS 操作DOM"></a>传统 JS 操作DOM</h3><p>传统的JS直接写代码逻辑去操作DOM，原生JS：</p><pre><code class="html">&lt;!-- HTML --&gt;&lt;span id=&quot;name&quot;&gt;&lt;/span&gt;</code></pre><pre><code class="js">var dom = document.getElementById(&#39;name&#39;);dom.innerHTML = &#39;Sansan&#39;;dom.style.color = &#39;red&#39;</code></pre><h3 id="jQuery-修改-DOM"><a href="#jQuery-修改-DOM" class="headerlink" title="jQuery 修改 DOM"></a>jQuery 修改 DOM</h3><pre><code class="html">&lt;!-- HTML --&gt;&lt;span id=&quot;name&quot;&gt;&lt;/span&gt;</code></pre><p>用 jQuery 修改 DOM：</p><pre><code class="js">$(&#39;#name&#39;).text(&quot;Sansan&quot;).css(&quot;color&quot;, &quot;red&quot;);</code></pre><p>jQuery中数据处理的逻辑和视图混合在一起，并未分离。</p><h3 id="MVVM-修改-DOM"><a href="#MVVM-修改-DOM" class="headerlink" title="MVVM 修改 DOM"></a>MVVM 修改 DOM</h3><p>只需要关注数据结构，由 ViewModel 进行双向绑定数据操作：</p><pre><code class="html">&lt;!-- HTML --&gt;&lt;div id=&quot;app&quot;&gt;    &lt;input v-model=&quot;name&quot; /&gt;    &lt;span&gt;姓名:{{name}}&lt;/span&gt;    &lt;span&gt;年龄:{{age}}&lt;/span&gt;&lt;/div&gt;</code></pre><p>Vue.js</p><pre><code class="js">var app = new Vue({    el: &#39;#app&#39;,    data: {        name: &#39;Sansan&#39;,        age: 20    }})</code></pre><p>Vue.js 将数据视图分离，以数据驱动视图，只关心数据变化，DOM操作被封装。</p><h2 id="耦合与解耦"><a href="#耦合与解耦" class="headerlink" title="耦合与解耦"></a>耦合与解耦</h2><h3 id="耦合"><a href="#耦合" class="headerlink" title="耦合"></a>耦合</h3><p>软件工程中，对象之间的耦合度就是对象之间的依赖性。对象之间耦合越高，维护成本越高，因此对象的设计应该使类和构件之间的耦合最小。耦合性用来衡量程序结构中各个模块之间的相互关联，同时取决于各个模块之间接口的复杂程度、调用模块的方式是什么，以及哪些信息通过接口。</p><h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>解耦即尽可能减少代码之间的耦合，数据、业务和视图之间尽可能的降低耦合。</p><p>原则就是A功能的代码不要写在B的功能代码中，如果两者之间需要交互，可以通过接口，通过消息，甚至可以引入框架，但总之就是不要直接交叉写。</p><p>在Vue中，解耦即将<strong>视图</strong>与<strong>数据</strong>分成两部分，即<strong>视图代码</strong>与<strong>业务逻辑</strong>的解耦。</p><hr><p><strong>参考来源：</strong></p><ul><li><em><a href="https://www.liaoxuefeng.com/wiki/1022910821149312/1108898947791072" target="_blank" rel="noopener">廖雪峰的官方网站</a></em></li><li><em><a href="https://blog.csdn.net/zhanghuali0210/article/details/82287544?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">vue考点 —— MVVM</a></em></li><li><em><a href="https://blog.csdn.net/u012551928/article/details/99545791" target="_blank" rel="noopener">MVVM模型</a></em></li><li><em><a href="https://blog.csdn.net/shenwansan_gz/article/details/82284957" target="_blank" rel="noopener">什么是耦合、解耦</a></em></li></ul>]]></content>
    
    
    <categories>
      
      <category>前端开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>知识点</tag>
      
      <tag>Vue.js</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>拥抱VS Code，抛弃Atom</title>
    <link href="/2020/03/20/vscode/"/>
    <url>/2020/03/20/vscode/</url>
    
    <content type="html"><![CDATA[<blockquote><p>没捣鼓过几个代码编辑器，都不好意思说自己写过代码。</p></blockquote><p>留学的时候，因为开源和颜值，爱上了Atom，期间不管它多少次龟速加载我都毫无怨言，teletype好用，暗黑主题真香。<a id="more"></a> 至于Sublime Text，因为当初觉得界面太丑，用了一次就放弃了。</p><p>可是，前几天看到VS Code、Sublime和Atom的趋势图，VS Code的势头在这两年增长很迅猛。经过两年的更新之后，VS Code的功能已经非常健全，加上速度快不卡顿，再加上还能下载Atom的主题插件，于是我就叛逃到VS Code，卸载了Atom 🙈。</p><p>经过一番折腾，记录下目前觉得实用的插件：</p><h2 id="One-Dark-Pro主题"><a href="#One-Dark-Pro主题" class="headerlink" title="One Dark Pro主题"></a>One Dark Pro主题</h2><p><img src="/img/one_dark_pro.jpg" srcset="/img/loading.gif" alt="主题"></p><p>Atom里我最爱的主题，留学期间的作业都是依靠这简洁而又不做作的主题，给我了无数次爬起来修复bug的力量。暗黑主题，配色很舒服，我个人非常喜欢。</p><p>One Dark Pro这个主题的配色，相比原版要稍微鲜艳一些，看了一下配置文件里的颜色代码一致，但呈现出来的效果却有一些差别。自己捣鼓了几版配色觉得还是不太满意，暂且先用默认的了。部分类别配色的设置红色偏多，稍稍做了一些修改，显得整体的红配绿不那么突兀。</p><h2 id="Color-Highlight"><a href="#Color-Highlight" class="headerlink" title="Color Highlight"></a>Color Highlight</h2><p>这个插件可以把颜色代码显示出对应的颜色，不需要依靠页面预览才能看到效果，调试CSS配色的时候更加方便，节省时间。</p><h2 id="Live-Server"><a href="#Live-Server" class="headerlink" title="Live Server"></a>Live Server</h2><img src="/img/live.jpg" srcset="/img/loading.gif" width="30%"><p>如果是前端开发，这个插件就很有用了，可以在浏览器实时显示HTML页面。最近在学习Vue，简单的例子都可以在网页里localhost实时预览，不要太方便。</p><h2 id="Vetur"><a href="#Vetur" class="headerlink" title="Vetur"></a>Vetur</h2><p>如果学习Vue，这个插件就是必备了。另外还需要一些其他的配置，后面Vue学习的部分再来细说。</p><p>另外还有代码格式美化、minimap（自带）、terminal（自带）都是编辑器里很好的助手。必需的插件配置好之后，就可以愉快的coding了。另外，VS Code也支持Python相关的环境配置，比如Jupyter Notebook之类的，我暂时还没尝试。</p><p>初步体验下来，VS Code确实像You大说的一样，真香。</p>]]></content>
    
    
    <categories>
      
      <category>前端开发</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020年的一些计划</title>
    <link href="/2020/01/08/2020%E5%B9%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A1%E5%88%92/"/>
    <url>/2020/01/08/2020%E5%B9%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<p>经过一天的时间搭建Hexo，我的个人博客地址有个了雏形，目前还在找背景图片的阶段，再过几天才能整成想要的样子。</p><p>不得不说，Hexo这类工具真的蛮好用，虽然前期花时间部署比较费劲，但是呈现的效果还是非常不错的，再加上自己可以在原有主题的基础上不断折腾，也是乐趣所在。</p><a id="more"></a><p>第一篇博客主要想把2020年我的计划写下来，经常翻看，保持计划的进度。</p><h2 id="2020年计划"><a href="#2020年计划" class="headerlink" title="2020年计划"></a>2020年计划</h2><h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><p>2020年的计划主要是四个方面：</p><ol><li>搭建个人技术博客 （进行中）</li><li>完成一个数据可视化网站项目</li><li>完成一个微信小程序项目</li><li>微信号/百家号</li></ol><p>希望能在今年把这四件事情按顺序一一搭建起来，即使无法做到完成，也希望能开始行动起来，先找到idea，再把整体思路和框架梳理出来。</p><h3 id="关于个人技术博客"><a href="#关于个人技术博客" class="headerlink" title="关于个人技术博客"></a>关于个人技术博客</h3><p>这部分一直想做，但是没腾出时间来好好研究。最开始还是以WordPress来研究，但由于当时觉得部署麻烦，就没继续。后来做了前端的一些项目之后，生出了自己开发博客网站的想法，无意中在知乎上看到了现在已经如此普遍的Hexo、Hugo等等工具，茅塞顿开。这正是我想要的么！</p><p>那为什么没选Hugo？由于Hexo是基于JavaScript的，Hugo是基于Go语言的，前者对我来说更加熟悉，如果想要在原有基础上折腾，也有很多可以发挥的空间，于是就开开心心地花了一天时间部署到了GitHub Page上。基础工作就算完成了。</p><p>第一版的大概长这样：</p><p><img src="/img/3.jpg" srcset="/img/loading.gif" alt="界面"></p><p>熟悉的 “Hello World”。</p><p>接下来就是确定一下更新的计划了。博客基本根据个人的学习情况和前端开发项目的进展情况来更新，知识的总结、遇到的问题以及其他的感悟放到这上面，作为记录。好记性不如烂笔头。</p><h3 id="关于数据可视化网站项目"><a href="#关于数据可视化网站项目" class="headerlink" title="关于数据可视化网站项目"></a>关于数据可视化网站项目</h3><p>元旦的时候，我原本想把之前写的一个爬虫程序抓到的数据作为这个项目的数据源，不过因为爬虫过程还不完善，而且有些地方仍然需要手动干预，所以关于这个项目的构想就暂时搁置了。</p><p>仔细想想，抛开爬虫实时抓取的需求，仅仅把过往累积的数据拿出来做展示也未尝不可。只不过这样一来，没有办法与爬虫程序整合，只能依靠本地抓取到的数据进行可视化，交互性和功能性会差很多。</p><p>春节期间，再好好考虑一下这个项目。</p><h3 id="关于微信小程序项目"><a href="#关于微信小程序项目" class="headerlink" title="关于微信小程序项目"></a>关于微信小程序项目</h3><p>这个项目需要整体学习小程序开发的语言，目前面临几个问题：第一，开发什么，目前还没有认真思考过，没有清晰的idea；第二，其他的项目在前，这个项目可能要往后面排了，现在时间精力都没办法让我好好想关于这个项目的东西。</p><p>所以，Flag就先立在这吧，回头再来拔。。</p><h3 id="微信号-百家号"><a href="#微信号-百家号" class="headerlink" title="微信号/百家号"></a>微信号/百家号</h3><p>自媒体这部分其实开始了很长时间，但是没能坚持下来，断断续续更新，还没得原创头衔，稍显动力不足。</p><p>春节期间的计划是，拿另一个身份重新注册一个公众号，把百家号上发过的文章拿到公众号的平台上重新发一遍。</p><p>其次，争取恢复百家号的日常更新，考虑到自己犯懒，先定在每周末更新一到两篇。</p><p>每年都是Flag自豪的立起来，年终的时候大部分倒下。可能这就是三三同学生活的常态吧。可三三同学也不应该当一条咸鱼，还是要奋力折腾啊！</p><p><strong>がんばって！</strong></p>]]></content>
    
    
    <categories>
      
      <category>计划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>个人</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
